{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hey! it's Map Action Map Action envisions a world where technology revolutionizes environmental management and rural & urban problem-solving, making sustainable living accessible to all communities in Mali and beyond. In a world increasingly challenged by environmental issues and urban complexities, Map Action envisions a future where cutting-edge technology and geospatial solutions empower communities, governments, and organizations. Our vision is to create a global society where sustainable urban and rural development and environmental stewardship are not only achievable but are the cornerstones of our collective well-being. Mission Statement Our mission is to develop, deploy, and promote open-source mapping tools and methodologies that enable individuals, communities, governments, and organizations to collaboratively identify, analyze, and solve critical environmental and urban challenges. Community Statement Map Action thrives on the strength of a diverse, inclusive community united by the goal of using technology for sustainable urban and environmental management. We commit to fostering an open, respectful environment where every voice is valued, and collaboration drives innovation. Together, we empower individuals and organizations to actively participate in crafting solutions that make a meaningful impact.","title":"Welcom"},{"location":"#hey-its-map-action","text":"Map Action envisions a world where technology revolutionizes environmental management and rural & urban problem-solving, making sustainable living accessible to all communities in Mali and beyond. In a world increasingly challenged by environmental issues and urban complexities, Map Action envisions a future where cutting-edge technology and geospatial solutions empower communities, governments, and organizations. Our vision is to create a global society where sustainable urban and rural development and environmental stewardship are not only achievable but are the cornerstones of our collective well-being.","title":"Hey! it's Map Action"},{"location":"contrubition-guideline/","text":"Map Action Contribution Guidelines Welcome to Map Action! We're excited about your interest in contributing to our open-source project. These guidelines will help you understand how to effectively contribute to our codebase. About Map Action Map Action is a Bamako-based trailblazer in using mapping technology to tackle environmental challenges and solve urban issues. Our innovative approach began by addressing Water, Sanitation, and Hygiene (WASH) problems and has grown to encompass various sectors through collaboration with civil society, governments, NGOs, and private entities. Our sustainable business model includes paid subscriptions for organizations that rely on our data to gain actionable insights across diverse domains. This model allows us to continuously enhance our offerings. Contributing We welcome contributions of all shapes and sizes! Here are the different ways you can get involved: Report bugs and request features: Identify any issues you encounter while using our tools. You can report them directly on GitHub Issues https://docs.github.com/en/issues/tracking-your-work-with-issues/about-issues. Submit code changes: If you have improvements or new features in mind, you can contribute code by creating pull requests (PRs). Write documentation or tutorials: Enhance our documentation to make it easier for others to understand and use our tools. Help with code reviews and testing: Lend your expertise by reviewing pull requests submitted by others and assisting with testing efforts. Finding Issues to Work On Browse our issue tracker on GitHub Issues https://docs.github.com/en/issues/tracking-your-work-with-issues/about-issues to find existing issues. Look for issues labeled \"help wanted\" or those categorized as bugs or enhancements that you're interested in tackling. Making Changes Fork the Repository: Visit the Map Action project on GitHub: https://github.com/223MapAction Click on the \"Fork\" button to create your own copy of the repository. Create a Branch: Clone your forked repository to your local machine. Create a new branch for your specific changes. Use a descriptive branch name that reflects your contribution (e.g., \"fix-map-loading-bug\"). Code and Commit: Make your changes to the codebase. Write clear and concise commit messages that describe your modifications. Testing: Ensure your changes don't introduce any regressions. We use pytest and flake8 for testing. Make sure your code passes all tests before submitting a pull request. Submitting Pull Requests Push to Your Branch: Once you're satisfied with your changes, push your local branch to your forked repository on GitHub. Open a Pull Request: Navigate to your forked repository on GitHub and go to the \"Pull Requests\" tab. Click on \"New pull request\" and select the branch containing your changes. Create a pull request with a clear and descriptive title and explanation of your modifications. Mention any issues your pull request addresses. Code Review: Our internal code review process involves two Map Action developers. They'll review your pull request and provide feedback or suggestions for improvement. Merge: Once your pull request is approved and any necessary changes are made, it will be merged into the main codebase. Additional Notes License: All our repositories use the GPL-3.0 license. Ensure your contributions comply with the license terms. Code of Conduct: We value a respectful and inclusive environment. Please familiarize yourself with our [Code of Conduct](CODE_OF_CONDUCT.md) before contributing. Appreciation: We appreciate all contributions, regardless of their scope. Thank you for helping us improve Map Action! We look forward to your contributions! This tailored guide incorporates the information you provided, making it specific to the Map Action project and its contribution workflow.","title":"Contrubition-guideline"},{"location":"dataFetching/","text":"Data fetching with dagshub Code snippet in dagshub_data_load.py downloads and organizes data from a CSV file and DagsHub repository for machine learning model training in the Map-Action-Model repository architecture. Environment Variables : - DAGSHUB_REPO_OWNER : Owner of the repository. - DAGSHUB_REPO : Repository name. - DATASOURCE_NAME : Name of the datasource configured in DagsHub. The dataset is divided as follows: - Training Data : 70% - Validation Data : 20% - Test Data : 10% download_and_organize_data () Download and organize data from a CSV file and DagsHub repository. Returns: Type Description Tuple [ Annotated [ str , train_dir ], Annotated [ str , valid_dir ], Annotated [ str , test_dir ], Annotated [ int , batch_size ]] Tuple[str, str, str, int]: Directories for train, valid, and test, and batch size. Source code in code/steps/dagshub_utils/dagshub_data_load.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 @step def download_and_organize_data () -> Tuple [ Annotated [ str , \"train_dir\" ], Annotated [ str , \"valid_dir\" ], Annotated [ str , \"test_dir\" ], Annotated [ int , \"batch_size\" ], ]: \"\"\" Download and organize data from a CSV file and DagsHub repository. Returns: Tuple[str, str, str, int]: Directories for train, valid, and test, and batch size. \"\"\" # Load data from a CSV file ds = datasources . get ( DAGSHUB_FULL_REPO , os . environ . get ( \"DATASOURCE_NAME\" )) ds = ds . all () . dataframe print ( ds ) df = pd . read_csv ( \"project-8.csv\" , usecols = [ \"choice\" , \"image\" ]) df_img = df [ \"image\" ] ds_img = ds [ \"path\" ] # Create directories for train, valid, and test data_dir = \"data\" train_dir = \"train\" valid_dir = \"valid\" test_dir = \"test\" os . makedirs ( data_dir ) # Randomly permute indices data_volumes = rand_shuffle = np . random . permutation ( df_img . shape [ 0 ]) print ( data_volumes ) print ( ds_img . shape [ 0 ]) # Download images and organize them into directories for n in range ( df_img . shape [ 0 ]): for m in range ( ds_img . shape [ 0 ]): if df_img [ n ] . split ( '/' )[ - 1 ] == ds_img [ m ]: save_dir = os . path . join ( data_dir , df [ 'choice' ][ n ]) try : os . makedirs ( save_dir ) except FileExistsError : pass # Directory already exists response = requests . get ( df_img [ n ], stream = True ) with open ( f \" { save_dir } /images { n + 1 } .jpg\" , 'wb' ) as file : for chunk in response . iter_content ( chunk_size = 128 ): file . write ( chunk ) print ( f \"File downloaded successfully in { n + 1 } \" ) # Gather file paths types = ( '*.jpg' , '*.jpeg' , '*.png' ) files = [] for ext in types : f = glob ( os . path . join ( data_dir , '*/' + ext )) files += f print ( files ) # Organize files into train, valid, and test directories \"\"\" for n in data_volumes[:10]: folder = files[n].split('/')[1] name = files[n].split('/')[-1] try: os.makedirs(os.path.join(data_dir, valid_dir, folder)) except FileExistsError: pass # Directory already exists os.rename(files[n], os.path.join(data_dir, valid_dir, folder, name)) \"\"\" for n in data_volumes [: 30 ]: folder = files [ n ] . split ( '/' )[ 1 ] name = files [ n ] . split ( '/' )[ - 1 ] try : os . makedirs ( os . path . join ( data_dir , test_dir , folder )) except FileExistsError : pass # Directory already exists os . rename ( files [ n ], os . path . join ( data_dir , test_dir , folder , name )) for n in data_volumes [ 30 :]: folder = files [ n ] . split ( '/' )[ 1 ] name = files [ n ] . split ( '/' )[ - 1 ] try : os . makedirs ( os . path . join ( data_dir , train_dir , folder )) except FileExistsError : pass # Directory already exists os . rename ( files [ n ], os . path . join ( data_dir , train_dir , folder , name )) print ( f \" { data_dir } / { train_dir } \" ) train_dir = f \" { data_dir } / { train_dir } \" valid_dir = f \" { data_dir } / { valid_dir } \" test_dir = f \" { data_dir } / { test_dir } \" batch_size = 20 return train_dir , valid_dir , test_dir , batch_size","title":"Data fetching"},{"location":"dataFetching/#data-fetching-with-dagshub","text":"Code snippet in dagshub_data_load.py downloads and organizes data from a CSV file and DagsHub repository for machine learning model training in the Map-Action-Model repository architecture. Environment Variables : - DAGSHUB_REPO_OWNER : Owner of the repository. - DAGSHUB_REPO : Repository name. - DATASOURCE_NAME : Name of the datasource configured in DagsHub. The dataset is divided as follows: - Training Data : 70% - Validation Data : 20% - Test Data : 10%","title":"Data fetching with dagshub"},{"location":"dataFetching/#steps.dagshub_utils.dagshub_data_load.download_and_organize_data","text":"Download and organize data from a CSV file and DagsHub repository. Returns: Type Description Tuple [ Annotated [ str , train_dir ], Annotated [ str , valid_dir ], Annotated [ str , test_dir ], Annotated [ int , batch_size ]] Tuple[str, str, str, int]: Directories for train, valid, and test, and batch size. Source code in code/steps/dagshub_utils/dagshub_data_load.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 @step def download_and_organize_data () -> Tuple [ Annotated [ str , \"train_dir\" ], Annotated [ str , \"valid_dir\" ], Annotated [ str , \"test_dir\" ], Annotated [ int , \"batch_size\" ], ]: \"\"\" Download and organize data from a CSV file and DagsHub repository. Returns: Tuple[str, str, str, int]: Directories for train, valid, and test, and batch size. \"\"\" # Load data from a CSV file ds = datasources . get ( DAGSHUB_FULL_REPO , os . environ . get ( \"DATASOURCE_NAME\" )) ds = ds . all () . dataframe print ( ds ) df = pd . read_csv ( \"project-8.csv\" , usecols = [ \"choice\" , \"image\" ]) df_img = df [ \"image\" ] ds_img = ds [ \"path\" ] # Create directories for train, valid, and test data_dir = \"data\" train_dir = \"train\" valid_dir = \"valid\" test_dir = \"test\" os . makedirs ( data_dir ) # Randomly permute indices data_volumes = rand_shuffle = np . random . permutation ( df_img . shape [ 0 ]) print ( data_volumes ) print ( ds_img . shape [ 0 ]) # Download images and organize them into directories for n in range ( df_img . shape [ 0 ]): for m in range ( ds_img . shape [ 0 ]): if df_img [ n ] . split ( '/' )[ - 1 ] == ds_img [ m ]: save_dir = os . path . join ( data_dir , df [ 'choice' ][ n ]) try : os . makedirs ( save_dir ) except FileExistsError : pass # Directory already exists response = requests . get ( df_img [ n ], stream = True ) with open ( f \" { save_dir } /images { n + 1 } .jpg\" , 'wb' ) as file : for chunk in response . iter_content ( chunk_size = 128 ): file . write ( chunk ) print ( f \"File downloaded successfully in { n + 1 } \" ) # Gather file paths types = ( '*.jpg' , '*.jpeg' , '*.png' ) files = [] for ext in types : f = glob ( os . path . join ( data_dir , '*/' + ext )) files += f print ( files ) # Organize files into train, valid, and test directories \"\"\" for n in data_volumes[:10]: folder = files[n].split('/')[1] name = files[n].split('/')[-1] try: os.makedirs(os.path.join(data_dir, valid_dir, folder)) except FileExistsError: pass # Directory already exists os.rename(files[n], os.path.join(data_dir, valid_dir, folder, name)) \"\"\" for n in data_volumes [: 30 ]: folder = files [ n ] . split ( '/' )[ 1 ] name = files [ n ] . split ( '/' )[ - 1 ] try : os . makedirs ( os . path . join ( data_dir , test_dir , folder )) except FileExistsError : pass # Directory already exists os . rename ( files [ n ], os . path . join ( data_dir , test_dir , folder , name )) for n in data_volumes [ 30 :]: folder = files [ n ] . split ( '/' )[ 1 ] name = files [ n ] . split ( '/' )[ - 1 ] try : os . makedirs ( os . path . join ( data_dir , train_dir , folder )) except FileExistsError : pass # Directory already exists os . rename ( files [ n ], os . path . join ( data_dir , train_dir , folder , name )) print ( f \" { data_dir } / { train_dir } \" ) train_dir = f \" { data_dir } / { train_dir } \" valid_dir = f \" { data_dir } / { valid_dir } \" test_dir = f \" { data_dir } / { test_dir } \" batch_size = 20 return train_dir , valid_dir , test_dir , batch_size","title":"download_and_organize_data"},{"location":"dataPreprocessing/","text":"Data Loading and bacth creation Code Summary**:data_loading_pipeline.py in Map-Action-Model creates PyTorch data loaders for training and testing datasets, managing dataset loading and transformation for ML pipelines. create_dataloaders ( train_dir , valid_dir , test_dir , batch_size ) Create PyTorch data loaders for training and testing datasets. Parameters: Name Type Description Default train_dir str Directory containing the training dataset. required valid_dir str Directory containing the validation dataset (currently commented out). required test_dir str Directory containing the testing dataset. required batch_size int Batch size for the data loaders. required Returns: Type Description Tuple [ Annotated [ DataLoader , training_dataloader ], Annotated [ DataLoader , testing_dataloader ], Annotated [ int , num_classes ], Annotated [ int , epochs ]] Tuple[DataLoader, DataLoader, int, int]: Tuple containing the training data loader, testing data loader, number of classes, and the number of epochs. Source code in code/steps/data_preprocess/data_loading_pipeline.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 @step def create_dataloaders ( train_dir : str , valid_dir : str , test_dir : str , batch_size : int ) -> Tuple [ Annotated [ DataLoader , \"training_dataloader\" ], Annotated [ DataLoader , \"testing_dataloader\" ], Annotated [ int , \"num_classes\" ], Annotated [ int , \"epochs\" ], ]: \"\"\" Create PyTorch data loaders for training and testing datasets. Args: train_dir (str): Directory containing the training dataset. valid_dir (str): Directory containing the validation dataset (currently commented out). test_dir (str): Directory containing the testing dataset. batch_size (int): Batch size for the data loaders. Returns: Tuple[DataLoader, DataLoader, int, int]: Tuple containing the training data loader, testing data loader, number of classes, and the number of epochs. \"\"\" NUM_WORKERS = 2 # Use ImageFolder to create dataset(s) train_data = datasets . ImageFolder ( train_dir , transform = get_transform ( train = True )) # valid_data = datasets.ImageFolder(valid_dir, transform=get_transform(train=True)) test_data = datasets . ImageFolder ( test_dir , transform = get_transform ( train = True )) # Get class names class_names = train_data . classes num_classes = len ( class_names ) # Turn images into data loaders training_dataloader = DataLoader ( train_data , batch_size = batch_size , shuffle = True , num_workers = NUM_WORKERS , pin_memory = True ) \"\"\" validation_dataloader = DataLoader( valid_data, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True ) \"\"\" testing_dataloader = DataLoader ( test_data , batch_size = batch_size , shuffle = False , num_workers = NUM_WORKERS , pin_memory = True ) epochs = 5 return training_dataloader , testing_dataloader , num_classes , epochs Data preprocessing Role:** Code snippet in data_transform.py for image preprocessing in Map-Action-Model repo architecture.Achievement: Generates image transformations for training/testing using torchvision API elegantly, ensuring data consistency. get_transform ( train ) Get image transformations based on whether it's for training or not. This function builds a series of image transformations using torchvision v2 API, tailored for either training or testing phase. The transformations for training include random horizontal flipping and resizing with cropping, followed by normalization. For both training and testing, the images are converted to torch.float32 and scaled. Parameters: Name Type Description Default train bool True if transformations are for training, False otherwise. required Returns: Type Description Compose T.Compose: A torchvision.transforms.Compose object that represents the composition Compose of image transformations. Source code in code/steps/data_preprocess/data_transform.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def get_transform ( train : bool ) -> T . Compose : \"\"\" Get image transformations based on whether it's for training or not. This function builds a series of image transformations using torchvision v2 API, tailored for either training or testing phase. The transformations for training include random horizontal flipping and resizing with cropping, followed by normalization. For both training and testing, the images are converted to torch.float32 and scaled. Args: train (bool): True if transformations are for training, False otherwise. Returns: T.Compose: A torchvision.transforms.Compose object that represents the composition of image transformations. \"\"\" transforms = [] # Initialize an empty list to store transformations if train : transforms . append ( T . RandomHorizontalFlip ( 0.5 )) # Randomly flip the image horizontally transforms . append ( T . RandomResizedCrop ( size = [ 224 , 224 ], antialias = True )) # Resize and crop transforms . append ( T . ToDtype ( torch . float32 , scale = True )) # Convert to float32 and scale return T . Compose ( transforms ) # Combine transformations into a Compose object","title":"Data pre-processing"},{"location":"dataPreprocessing/#data-loading-and-bacth-creation","text":"Code Summary**:data_loading_pipeline.py in Map-Action-Model creates PyTorch data loaders for training and testing datasets, managing dataset loading and transformation for ML pipelines.","title":"Data Loading and bacth creation"},{"location":"dataPreprocessing/#steps.data_preprocess.data_loading_pipeline.create_dataloaders","text":"Create PyTorch data loaders for training and testing datasets. Parameters: Name Type Description Default train_dir str Directory containing the training dataset. required valid_dir str Directory containing the validation dataset (currently commented out). required test_dir str Directory containing the testing dataset. required batch_size int Batch size for the data loaders. required Returns: Type Description Tuple [ Annotated [ DataLoader , training_dataloader ], Annotated [ DataLoader , testing_dataloader ], Annotated [ int , num_classes ], Annotated [ int , epochs ]] Tuple[DataLoader, DataLoader, int, int]: Tuple containing the training data loader, testing data loader, number of classes, and the number of epochs. Source code in code/steps/data_preprocess/data_loading_pipeline.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 @step def create_dataloaders ( train_dir : str , valid_dir : str , test_dir : str , batch_size : int ) -> Tuple [ Annotated [ DataLoader , \"training_dataloader\" ], Annotated [ DataLoader , \"testing_dataloader\" ], Annotated [ int , \"num_classes\" ], Annotated [ int , \"epochs\" ], ]: \"\"\" Create PyTorch data loaders for training and testing datasets. Args: train_dir (str): Directory containing the training dataset. valid_dir (str): Directory containing the validation dataset (currently commented out). test_dir (str): Directory containing the testing dataset. batch_size (int): Batch size for the data loaders. Returns: Tuple[DataLoader, DataLoader, int, int]: Tuple containing the training data loader, testing data loader, number of classes, and the number of epochs. \"\"\" NUM_WORKERS = 2 # Use ImageFolder to create dataset(s) train_data = datasets . ImageFolder ( train_dir , transform = get_transform ( train = True )) # valid_data = datasets.ImageFolder(valid_dir, transform=get_transform(train=True)) test_data = datasets . ImageFolder ( test_dir , transform = get_transform ( train = True )) # Get class names class_names = train_data . classes num_classes = len ( class_names ) # Turn images into data loaders training_dataloader = DataLoader ( train_data , batch_size = batch_size , shuffle = True , num_workers = NUM_WORKERS , pin_memory = True ) \"\"\" validation_dataloader = DataLoader( valid_data, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True ) \"\"\" testing_dataloader = DataLoader ( test_data , batch_size = batch_size , shuffle = False , num_workers = NUM_WORKERS , pin_memory = True ) epochs = 5 return training_dataloader , testing_dataloader , num_classes , epochs","title":"create_dataloaders"},{"location":"dataPreprocessing/#data-preprocessing","text":"Role:** Code snippet in data_transform.py for image preprocessing in Map-Action-Model repo architecture.Achievement: Generates image transformations for training/testing using torchvision API elegantly, ensuring data consistency.","title":"Data preprocessing"},{"location":"dataPreprocessing/#steps.data_preprocess.data_transform.get_transform","text":"Get image transformations based on whether it's for training or not. This function builds a series of image transformations using torchvision v2 API, tailored for either training or testing phase. The transformations for training include random horizontal flipping and resizing with cropping, followed by normalization. For both training and testing, the images are converted to torch.float32 and scaled. Parameters: Name Type Description Default train bool True if transformations are for training, False otherwise. required Returns: Type Description Compose T.Compose: A torchvision.transforms.Compose object that represents the composition Compose of image transformations. Source code in code/steps/data_preprocess/data_transform.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def get_transform ( train : bool ) -> T . Compose : \"\"\" Get image transformations based on whether it's for training or not. This function builds a series of image transformations using torchvision v2 API, tailored for either training or testing phase. The transformations for training include random horizontal flipping and resizing with cropping, followed by normalization. For both training and testing, the images are converted to torch.float32 and scaled. Args: train (bool): True if transformations are for training, False otherwise. Returns: T.Compose: A torchvision.transforms.Compose object that represents the composition of image transformations. \"\"\" transforms = [] # Initialize an empty list to store transformations if train : transforms . append ( T . RandomHorizontalFlip ( 0.5 )) # Randomly flip the image horizontally transforms . append ( T . RandomResizedCrop ( size = [ 224 , 224 ], antialias = True )) # Resize and crop transforms . append ( T . ToDtype ( torch . float32 , scale = True )) # Convert to float32 and scale return T . Compose ( transforms ) # Combine transformations into a Compose object","title":"get_transform"},{"location":"imageAnnotation/","text":"Image Annotation Process Overview This section outlines the process of annotating images using the DagsHub integration with Label Studio, a popular tool for data labeling tasks. The annotation process plays a crucial role in preparing data for training machine learning models by classifying images into predefined categories. Setting Up Label Studio with DagsHub Integration Overview Label Studio is integrated with DagsHub to streamline the annotation workflow and facilitate the management and storage of annotated datasets. This integration allows for seamless synchronization of annotation data with DagsHub repositories, making it easier to track and version annotation progress. Initial Setup To start annotating images using Label Studio within a DagsHub environment: Connect Your DagsHub Repository : Ensure that your DagsHub repository is set up to store and manage your annotation data. Configure the repository settings in Label Studio to point to your specific DagsHub repository. Configure Label Studio : Launch Label Studio and create a new project. Choose the type of task (image classification, object detection, etc.) and define the label classes. Configuring Annotation Classes For the task of image classification, define the following six classes which correspond to the categories required for your model: Class 1 Class 2 Class 3 Class 4 Class 5 Class 6 Each class should be clearly described to ensure that annotators understand the criteria for each classification. Annotation Process Annotating Images Upload Images : Images to be annotated are uploaded to the Label Studio project directly or synchronized from a designated folder in your DagsHub repository. Manual Annotation : Annotators classify each image by assigning one of the six predefined classes. Ensure each image is viewed and classified accurately to maintain high-quality data for model training. Quality Assurance Regular checks and reviews should be conducted to maintain the consistency and accuracy of the annotations. Discrepancies or uncertainties in image classification should be discussed and resolved to refine the annotation guidelines. Exporting Annotations Once the annotation process is complete, the annotated data can be exported from Label Studio into a CSV format, which is convenient for further processing and analysis in data preparation steps. Export Procedure Navigate to the export section in Label Studio. Select the CSV format for export. Download the annotated dataset, which will include image file references and their corresponding class labels. Conclusion The integration of Label Studio with DagsHub facilitates a robust framework for annotating images, ensuring that the data used for training machine learning models is accurately classified and easily accessible. This documentation provides a clear guide on how to manage the image annotation process effectively within this integrated environment.","title":"Image annotation process"},{"location":"imageAnnotation/#image-annotation-process","text":"","title":"Image Annotation Process"},{"location":"imageAnnotation/#overview","text":"This section outlines the process of annotating images using the DagsHub integration with Label Studio, a popular tool for data labeling tasks. The annotation process plays a crucial role in preparing data for training machine learning models by classifying images into predefined categories.","title":"Overview"},{"location":"imageAnnotation/#setting-up-label-studio-with-dagshub","text":"","title":"Setting Up Label Studio with DagsHub"},{"location":"imageAnnotation/#integration-overview","text":"Label Studio is integrated with DagsHub to streamline the annotation workflow and facilitate the management and storage of annotated datasets. This integration allows for seamless synchronization of annotation data with DagsHub repositories, making it easier to track and version annotation progress.","title":"Integration Overview"},{"location":"imageAnnotation/#initial-setup","text":"To start annotating images using Label Studio within a DagsHub environment: Connect Your DagsHub Repository : Ensure that your DagsHub repository is set up to store and manage your annotation data. Configure the repository settings in Label Studio to point to your specific DagsHub repository. Configure Label Studio : Launch Label Studio and create a new project. Choose the type of task (image classification, object detection, etc.) and define the label classes.","title":"Initial Setup"},{"location":"imageAnnotation/#configuring-annotation-classes","text":"For the task of image classification, define the following six classes which correspond to the categories required for your model: Class 1 Class 2 Class 3 Class 4 Class 5 Class 6 Each class should be clearly described to ensure that annotators understand the criteria for each classification.","title":"Configuring Annotation Classes"},{"location":"imageAnnotation/#annotation-process","text":"","title":"Annotation Process"},{"location":"imageAnnotation/#annotating-images","text":"Upload Images : Images to be annotated are uploaded to the Label Studio project directly or synchronized from a designated folder in your DagsHub repository. Manual Annotation : Annotators classify each image by assigning one of the six predefined classes. Ensure each image is viewed and classified accurately to maintain high-quality data for model training.","title":"Annotating Images"},{"location":"imageAnnotation/#quality-assurance","text":"Regular checks and reviews should be conducted to maintain the consistency and accuracy of the annotations. Discrepancies or uncertainties in image classification should be discussed and resolved to refine the annotation guidelines.","title":"Quality Assurance"},{"location":"imageAnnotation/#exporting-annotations","text":"Once the annotation process is complete, the annotated data can be exported from Label Studio into a CSV format, which is convenient for further processing and analysis in data preparation steps.","title":"Exporting Annotations"},{"location":"imageAnnotation/#export-procedure","text":"Navigate to the export section in Label Studio. Select the CSV format for export. Download the annotated dataset, which will include image file references and their corresponding class labels.","title":"Export Procedure"},{"location":"imageAnnotation/#conclusion","text":"The integration of Label Studio with DagsHub facilitates a robust framework for annotating images, ensuring that the data used for training machine learning models is accurately classified and easily accessible. This documentation provides a clear guide on how to manage the image annotation process effectively within this integrated environment.","title":"Conclusion"},{"location":"install/","text":"Setup and Running Process Overview This section outlines the steps required to set up and run the continuous training pipeline for the machine learning model using Docker Compose. The process involves configuring environment variables and utilizing a Docker Compose file to automate the deployment and operation of the training environment. Environment Setup Configuring Environment Variables Before launching the training pipeline, it is crucial to set up the required environment variables. These variables are essential for configuring the application's settings and ensuring the correct operation of the Docker containers. Create an .env file in the root directory of your project with the following structure: Environment configuration DAGSHUB_REPO_OWNER=your_dagshub_username DAGSHUB_REPO=your_repository_name DATASOURCE_NAME=your_datasource_name Running the Training Pipeline To run the continuous training pipeline, use the Docker Compose file specifically prepared for this purpose. The Docker Compose configuration manages the necessary services, volumes, and networks to facilitate the training process. Starting the Pipeline Run the following command in your terminal to start the training pipeline: docker-compose -f _ct.yml up","title":"Setup and Run"},{"location":"install/#setup-and-running-process","text":"","title":"Setup and Running Process"},{"location":"install/#overview","text":"This section outlines the steps required to set up and run the continuous training pipeline for the machine learning model using Docker Compose. The process involves configuring environment variables and utilizing a Docker Compose file to automate the deployment and operation of the training environment.","title":"Overview"},{"location":"install/#environment-setup","text":"","title":"Environment Setup"},{"location":"install/#configuring-environment-variables","text":"Before launching the training pipeline, it is crucial to set up the required environment variables. These variables are essential for configuring the application's settings and ensuring the correct operation of the Docker containers. Create an .env file in the root directory of your project with the following structure:","title":"Configuring Environment Variables"},{"location":"install/#environment-configuration","text":"DAGSHUB_REPO_OWNER=your_dagshub_username DAGSHUB_REPO=your_repository_name DATASOURCE_NAME=your_datasource_name","title":"Environment configuration"},{"location":"install/#running-the-training-pipeline","text":"To run the continuous training pipeline, use the Docker Compose file specifically prepared for this purpose. The Docker Compose configuration manages the necessary services, volumes, and networks to facilitate the training process.","title":"Running the Training Pipeline"},{"location":"install/#starting-the-pipeline","text":"Run the following command in your terminal to start the training pipeline: docker-compose -f _ct.yml up","title":"Starting the Pipeline"},{"location":"modelEvaluation/","text":"Model Inference Code Summary:**This code snippet performs testing for a PyTorch model, evaluating test data and logging metrics with MLFlow. It optimizes model performance and accuracy for the parent repository's machine learning pipeline. test_step ( model , test_dataloader , loss_fn , results , epochs ) Perform testing step for a PyTorch model. Parameters: Name Type Description Default model Module The PyTorch model to evaluate. required test_dataloader DataLoader DataLoader for the test dataset. required loss_fn Module Loss function for the evaluation. required results Dict Dictionary to store results. required epochs int Number of epochs. required Returns: Type Description Tuple [ Annotated [ float , test_loss ], Annotated [ float , test_acc ], Annotated [ Dict , results ]] Tuple[float, float, Dict]: Tuple containing test loss, test accuracy, and results. Source code in code/steps/model_eval/evaluation.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 @step ( experiment_tracker = \"mlflow_tracker-2\" ) def test_step ( model : nn . Module , test_dataloader : DataLoader , loss_fn : nn . Module , results : Dict , epochs : int ) -> Tuple [ Annotated [ float , \"test_loss\" ], Annotated [ float , \"test_acc\" ], Annotated [ Dict , \"results\" ] ]: \"\"\" Perform testing step for a PyTorch model. Args: model (nn.Module): The PyTorch model to evaluate. test_dataloader (DataLoader): DataLoader for the test dataset. loss_fn (nn.Module): Loss function for the evaluation. results (Dict): Dictionary to store results. epochs (int): Number of epochs. Returns: Tuple[float, float, Dict]: Tuple containing test loss, test accuracy, and results. \"\"\" device = torch . device ( 'cuda' ) if torch . cuda . is_available () else torch . device ( 'cpu' ) model . eval () test_loss , test_acc = 0 , 0 for epoch in tqdm ( range ( epochs )): with torch . inference_mode (): for batch , ( X , y ) in enumerate ( test_dataloader ): X , y = X . to ( device ), y . to ( device ) test_pred_logits = model ( X ) loss = loss_fn ( test_pred_logits , y ) test_loss += loss . cpu () . item () test_pred_labels = test_pred_logits . argmax ( dim = 1 ) test_acc += (( test_pred_labels == y ) . sum () . item () / len ( test_pred_labels )) test_loss = test_loss / len ( test_dataloader ) test_acc = test_acc / len ( test_dataloader ) results [ \"test_loss\" ] . append ( test_loss ) results [ \"test_acc\" ] . append ( test_acc ) mlflow . log_metric ( \"test_loss\" , test_loss ) mlflow . log_metric ( \"test_acc\" , test_acc ) # Log the PyTorch model as an artifact mlflow . pytorch . log_model ( model , \"model\" ) torch . save ( model . state_dict (), 'save/TCM1.pth' ) return test_loss , test_acc , results","title":"Model inference"},{"location":"modelEvaluation/#model-inference","text":"Code Summary:**This code snippet performs testing for a PyTorch model, evaluating test data and logging metrics with MLFlow. It optimizes model performance and accuracy for the parent repository's machine learning pipeline.","title":"Model Inference"},{"location":"modelEvaluation/#steps.model_eval.evaluation.test_step","text":"Perform testing step for a PyTorch model. Parameters: Name Type Description Default model Module The PyTorch model to evaluate. required test_dataloader DataLoader DataLoader for the test dataset. required loss_fn Module Loss function for the evaluation. required results Dict Dictionary to store results. required epochs int Number of epochs. required Returns: Type Description Tuple [ Annotated [ float , test_loss ], Annotated [ float , test_acc ], Annotated [ Dict , results ]] Tuple[float, float, Dict]: Tuple containing test loss, test accuracy, and results. Source code in code/steps/model_eval/evaluation.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 @step ( experiment_tracker = \"mlflow_tracker-2\" ) def test_step ( model : nn . Module , test_dataloader : DataLoader , loss_fn : nn . Module , results : Dict , epochs : int ) -> Tuple [ Annotated [ float , \"test_loss\" ], Annotated [ float , \"test_acc\" ], Annotated [ Dict , \"results\" ] ]: \"\"\" Perform testing step for a PyTorch model. Args: model (nn.Module): The PyTorch model to evaluate. test_dataloader (DataLoader): DataLoader for the test dataset. loss_fn (nn.Module): Loss function for the evaluation. results (Dict): Dictionary to store results. epochs (int): Number of epochs. Returns: Tuple[float, float, Dict]: Tuple containing test loss, test accuracy, and results. \"\"\" device = torch . device ( 'cuda' ) if torch . cuda . is_available () else torch . device ( 'cpu' ) model . eval () test_loss , test_acc = 0 , 0 for epoch in tqdm ( range ( epochs )): with torch . inference_mode (): for batch , ( X , y ) in enumerate ( test_dataloader ): X , y = X . to ( device ), y . to ( device ) test_pred_logits = model ( X ) loss = loss_fn ( test_pred_logits , y ) test_loss += loss . cpu () . item () test_pred_labels = test_pred_logits . argmax ( dim = 1 ) test_acc += (( test_pred_labels == y ) . sum () . item () / len ( test_pred_labels )) test_loss = test_loss / len ( test_dataloader ) test_acc = test_acc / len ( test_dataloader ) results [ \"test_loss\" ] . append ( test_loss ) results [ \"test_acc\" ] . append ( test_acc ) mlflow . log_metric ( \"test_loss\" , test_loss ) mlflow . log_metric ( \"test_acc\" , test_acc ) # Log the PyTorch model as an artifact mlflow . pytorch . log_model ( model , \"model\" ) torch . save ( model . state_dict (), 'save/TCM1.pth' ) return test_loss , test_acc , results","title":"test_step"},{"location":"modelFinetuning/","text":"Model fune tuning Code snippet in m_a_model.py creates a modified VGG16 model for a specific class count. It adjusts the classifier and uses CrossEntropyLoss. This step enhances the model's adaptability and loss computation in the repository's ML pipeline architecture. m_a_model ( num_classes ) Create a modified VGG16 model for a given number of classes. Parameters: Name Type Description Default num_classes int Number of classes for the classifier. required Returns: Type Description Tuple [ Annotated [ Module , model ], Annotated [ Module , loss_fn ]] Tuple[torch.nn.Module, torch.nn.Module]: Tuple containing the modified VGG16 model and the CrossEntropyLoss. Source code in code/steps/model/m_a_model.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 @step def m_a_model ( num_classes : int ) -> Tuple [ Annotated [ torch . nn . Module , \"model\" ], Annotated [ torch . nn . Module , \"loss_fn\" ] ]: \"\"\" Create a modified VGG16 model for a given number of classes. Args: num_classes (int): Number of classes for the classifier. Returns: Tuple[torch.nn.Module, torch.nn.Module]: Tuple containing the modified VGG16 model and the CrossEntropyLoss. \"\"\" # Load VGG16 model with batch normalization weights vgg16_bn_weights = VGG16_BN_Weights . DEFAULT model = vgg16_bn ( weights = vgg16_bn_weights ) # Freeze all parameters in the model for params in model . parameters (): params . requires_grad = False # Modify the classifier to adapt to the number of classes num_features = model . classifier [ 6 ] . in_features features = list ( model . classifier . children ())[: - 1 ] features . extend ([ torch . nn . Linear ( num_features , num_classes )]) model . classifier = torch . nn . Sequential ( * features ) # Define CrossEntropyLoss as the loss function loss_fn = torch . nn . CrossEntropyLoss () return model , loss_fn","title":"Model fine tuning"},{"location":"modelFinetuning/#model-fune-tuning","text":"Code snippet in m_a_model.py creates a modified VGG16 model for a specific class count. It adjusts the classifier and uses CrossEntropyLoss. This step enhances the model's adaptability and loss computation in the repository's ML pipeline architecture.","title":"Model fune tuning"},{"location":"modelFinetuning/#steps.model.m_a_model.m_a_model","text":"Create a modified VGG16 model for a given number of classes. Parameters: Name Type Description Default num_classes int Number of classes for the classifier. required Returns: Type Description Tuple [ Annotated [ Module , model ], Annotated [ Module , loss_fn ]] Tuple[torch.nn.Module, torch.nn.Module]: Tuple containing the modified VGG16 model and the CrossEntropyLoss. Source code in code/steps/model/m_a_model.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 @step def m_a_model ( num_classes : int ) -> Tuple [ Annotated [ torch . nn . Module , \"model\" ], Annotated [ torch . nn . Module , \"loss_fn\" ] ]: \"\"\" Create a modified VGG16 model for a given number of classes. Args: num_classes (int): Number of classes for the classifier. Returns: Tuple[torch.nn.Module, torch.nn.Module]: Tuple containing the modified VGG16 model and the CrossEntropyLoss. \"\"\" # Load VGG16 model with batch normalization weights vgg16_bn_weights = VGG16_BN_Weights . DEFAULT model = vgg16_bn ( weights = vgg16_bn_weights ) # Freeze all parameters in the model for params in model . parameters (): params . requires_grad = False # Modify the classifier to adapt to the number of classes num_features = model . classifier [ 6 ] . in_features features = list ( model . classifier . children ())[: - 1 ] features . extend ([ torch . nn . Linear ( num_features , num_classes )]) model . classifier = torch . nn . Sequential ( * features ) # Define CrossEntropyLoss as the loss function loss_fn = torch . nn . CrossEntropyLoss () return model , loss_fn","title":"m_a_model"},{"location":"modelTraining/","text":"Model training Code snippet in training_step.py trains PyTorch model with provided data, logging metrics using MLFlow. Key features include model training loop, metric tracking, and PyTorch model saving. test_step ( model , test_dataloader , loss_fn , results , epochs ) Perform testing step for a PyTorch model. Parameters: Name Type Description Default model Module The PyTorch model to evaluate. required test_dataloader DataLoader DataLoader for the test dataset. required loss_fn Module Loss function for the evaluation. required results Dict Dictionary to store results. required epochs int Number of epochs. required Returns: Type Description Tuple [ Annotated [ float , test_loss ], Annotated [ float , test_acc ], Annotated [ Dict , results ]] Tuple[float, float, Dict]: Tuple containing test loss, test accuracy, and results. Source code in code/steps/model_eval/evaluation.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 @step ( experiment_tracker = \"mlflow_tracker-2\" ) def test_step ( model : nn . Module , test_dataloader : DataLoader , loss_fn : nn . Module , results : Dict , epochs : int ) -> Tuple [ Annotated [ float , \"test_loss\" ], Annotated [ float , \"test_acc\" ], Annotated [ Dict , \"results\" ] ]: \"\"\" Perform testing step for a PyTorch model. Args: model (nn.Module): The PyTorch model to evaluate. test_dataloader (DataLoader): DataLoader for the test dataset. loss_fn (nn.Module): Loss function for the evaluation. results (Dict): Dictionary to store results. epochs (int): Number of epochs. Returns: Tuple[float, float, Dict]: Tuple containing test loss, test accuracy, and results. \"\"\" device = torch . device ( 'cuda' ) if torch . cuda . is_available () else torch . device ( 'cpu' ) model . eval () test_loss , test_acc = 0 , 0 for epoch in tqdm ( range ( epochs )): with torch . inference_mode (): for batch , ( X , y ) in enumerate ( test_dataloader ): X , y = X . to ( device ), y . to ( device ) test_pred_logits = model ( X ) loss = loss_fn ( test_pred_logits , y ) test_loss += loss . cpu () . item () test_pred_labels = test_pred_logits . argmax ( dim = 1 ) test_acc += (( test_pred_labels == y ) . sum () . item () / len ( test_pred_labels )) test_loss = test_loss / len ( test_dataloader ) test_acc = test_acc / len ( test_dataloader ) results [ \"test_loss\" ] . append ( test_loss ) results [ \"test_acc\" ] . append ( test_acc ) mlflow . log_metric ( \"test_loss\" , test_loss ) mlflow . log_metric ( \"test_acc\" , test_acc ) # Log the PyTorch model as an artifact mlflow . pytorch . log_model ( model , \"model\" ) torch . save ( model . state_dict (), 'save/TCM1.pth' ) return test_loss , test_acc , results","title":"Model training"},{"location":"modelTraining/#model-training","text":"Code snippet in training_step.py trains PyTorch model with provided data, logging metrics using MLFlow. Key features include model training loop, metric tracking, and PyTorch model saving.","title":"Model training"},{"location":"modelTraining/#steps.model_eval.evaluation.test_step","text":"Perform testing step for a PyTorch model. Parameters: Name Type Description Default model Module The PyTorch model to evaluate. required test_dataloader DataLoader DataLoader for the test dataset. required loss_fn Module Loss function for the evaluation. required results Dict Dictionary to store results. required epochs int Number of epochs. required Returns: Type Description Tuple [ Annotated [ float , test_loss ], Annotated [ float , test_acc ], Annotated [ Dict , results ]] Tuple[float, float, Dict]: Tuple containing test loss, test accuracy, and results. Source code in code/steps/model_eval/evaluation.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 @step ( experiment_tracker = \"mlflow_tracker-2\" ) def test_step ( model : nn . Module , test_dataloader : DataLoader , loss_fn : nn . Module , results : Dict , epochs : int ) -> Tuple [ Annotated [ float , \"test_loss\" ], Annotated [ float , \"test_acc\" ], Annotated [ Dict , \"results\" ] ]: \"\"\" Perform testing step for a PyTorch model. Args: model (nn.Module): The PyTorch model to evaluate. test_dataloader (DataLoader): DataLoader for the test dataset. loss_fn (nn.Module): Loss function for the evaluation. results (Dict): Dictionary to store results. epochs (int): Number of epochs. Returns: Tuple[float, float, Dict]: Tuple containing test loss, test accuracy, and results. \"\"\" device = torch . device ( 'cuda' ) if torch . cuda . is_available () else torch . device ( 'cpu' ) model . eval () test_loss , test_acc = 0 , 0 for epoch in tqdm ( range ( epochs )): with torch . inference_mode (): for batch , ( X , y ) in enumerate ( test_dataloader ): X , y = X . to ( device ), y . to ( device ) test_pred_logits = model ( X ) loss = loss_fn ( test_pred_logits , y ) test_loss += loss . cpu () . item () test_pred_labels = test_pred_logits . argmax ( dim = 1 ) test_acc += (( test_pred_labels == y ) . sum () . item () / len ( test_pred_labels )) test_loss = test_loss / len ( test_dataloader ) test_acc = test_acc / len ( test_dataloader ) results [ \"test_loss\" ] . append ( test_loss ) results [ \"test_acc\" ] . append ( test_acc ) mlflow . log_metric ( \"test_loss\" , test_loss ) mlflow . log_metric ( \"test_acc\" , test_acc ) # Log the PyTorch model as an artifact mlflow . pytorch . log_model ( model , \"model\" ) torch . save ( model . state_dict (), 'save/TCM1.pth' ) return test_loss , test_acc , results","title":"test_step"},{"location":"systemArch/","text":"System Arch Full code on github . Model Training pipeline Overall Arch of the system","title":"System Arch"},{"location":"systemArch/#system-arch","text":"Full code on github .","title":"System Arch"}]}